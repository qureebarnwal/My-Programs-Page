                1.1 Segmentation of objects in image sequences
It is very important in many aspects of multimedia applications. In second-generation image/video coding, images are segmented into objects to achieve efficient compression by coding the contour and texture separately. As the purpose is to achieve high compression performance, the objects segmented may not be semantically meaningful to human observers. The more recent applications, such as content-based image/video retrieval and image/video composition, require that the segmented objects be semantically meaningful. Indeed, the recent multimedia standard MPEG-4 specifies that a video is composed of meaningful video objects. Although many segmentation techniques have been proposed in the literature, fully automatic segmentation tools for general applications are currently not achievable. This is important and challenging area of segmentation of moving objects [1]. Finding moving objects in image sequences is one of the most important tasks in computer vision and image processing. For many years, the "obvious" approach has been first to compute the stationary background image, and then to identify the moving objects as those pixels in the image that differ significantly from the background. Let us call this the background subtraction approach. In earlier work as part of the Road watch project at Berkeley, it was shown that background subtraction can provide an effective means of locating and tracking moving vehicles in freeway traffic. Moving shadows do, however, cause serious problems, since they differ from the background  image and are therefore identified as parts of the moving objects. Moreover, when traffic is slow moving or stationary, the background image becomes corrupted by the vehicles themselves. These problems arise from an oversimplified view of the task. To classify each pixel of each image as moving object, shadow, or background. The basic idea is that to classify each pixel using a probabilistic model of how that pixel looks when it is part of different classes. For example, a given road pixel in shadow looks different from the same pixel in sunlight or as part of a vehicle. Because the appearance of the pixel in shadow is independent of the object that is casting the shadow, the shadow model for the pixel is relatively constant, like the background model. Furthermore, the probabilistic classification of the current pixel value can be used to update the models appropriately, so that vehicle pixels do not become mixed in with the background model when traffic is moving slowly. Let us show that our approach is successful in coping with slowmoving objects and shadows. There is a large literature on the application of expectation maximization and related techniques to image reconstruction, image segmentation, and motion identification [2].
                                1.2 Background Subtraction techniques
It is needed to compare various background subtraction algorithms for detecting moving vehicles and pedestrians in urban traffic video sequences. We considered approaches varying from simple techniques such as frame differencing and adaptive median filtering, to more sophisticated probabilistic modeling techniques. While complicated techniques often produce superior performance, our experiments show that simple techniques such as adaptive median filtering can produce good results with much lower computational complexity.                                                                                            II. ANALYSIS OF PROBLEM 
In the literature, many approaches for automatically adapting a background model to dynamic scene variations are proposed. Such methods differ mainly in the type of background model and in the procedure used to update the model. Due to the fast illumination changes such as: variations of the light, small movements of non-static objects, noise image in video sequences, and the results can be unreliable. An Accurate detection of moving objects is an important thing in tracking or recognition. A background subtraction method such as Gaussian mixture model (GMM) is one of representative methods used to detect moving objects. Image segmentation identifies the components of the image. It deals with sudden changes in light, changes in background object, presence of noise. Segmentation involves operations such as boundary detection, connected component labeling, thresholding. Our approach identifies and eliminates shadows much more effectively than other techniques like thresholding. Once foreground is extracted a simple subtraction operation can be used to extract the background.               
              III. PROPOSED WORK                         
            3.1. Segmentation: It is the process of identifying                                                                             components of the image. Segmentation involves operations such as boundary detection, connected component labeling, thresholding etc. Boundary detection finds out edges in the image. Any differential operator can be used for boundary detection [4, 5]. Thresholding is the process of reducing the grey levels in the image. Many algorithms exist for thresholding. Segmentation could be used for object recognition, occlusion boundary estimation within motion or stereo systems, image compression, image editing, or image database look-up.
                               3.2 Foreground extraction: As the name suggests
                                                                              this is the process of separating the foreground and background of the image. Here it is assumed that foreground contains the objects of interest. Some of the methods for foreground extraction are:Use of difference images -In this method we use subtraction of images in order to find objects that are moving and those that are not. The result of the subtraction is viewed as another grey image called difference image. Three types of difference images are defined [1].
• Absolute accumulative difference image is given by
f(x,y) = f(x,y) +1 ……………….if |g(x,y,ti+1) -
g(x,y,ti)| > T
• Positive accumulative difference image is given by
f(x,y) = f(x,y) +1 ……………….if g(x,y,ti+1) -
g(x,y,ti) > T
• Negative accumulative difference image is given
by
f(x,y) = f(x,y) +1 ……………….if g(x,y,ti) -
g(x,y,ti+1) > T
A gap-mountain method: It is applied to identify image blocks that are moving and those that are not moving. The gap-mountain method works as follows- Consider a difference image shown in the adjacent figure. A gap is a sequence of consecutive black pixels and mountain is a sequence ofconsecutive white pixels. If width of a mountain in a particular row  is greater than a preset threshold then we assume that a moving object is present in that row. Similar technique is the algorithm proceeds by dividing the image into smaller sub images (or sub matrices) until each sub matrix contains exactly one object. In the adjacent figure by choosing proper thresholds we can detect the presence of two blocks. Background extraction: Once foreground is extracted a simple subtraction operation can be used to extract the background [6]. Another method that can be used in object tracking is Background learning. This approach can be used when fixed cameras are used for video capturing. In this method, an initial training step is carried out before deploying the system. In the training step the system constantly records the background in order to ‗learn‘ it. Once the training is complete the system has complete (or almost complete) information about the background. Though this step is slightly lengthy, it has a very important advantage. Once we know the background, extracting the foreground is matter of simple image subtraction!
                           3.3 Feature extraction and object tracking:                                                                                                The next step is to extract useful features from the sequence of frames. Depending on the algorithm, definition of ‗feature‘ may vary. The next few sections explore some of the important techniques used for tracking: In the feature based approach discussed by Yiwei Wang, John Doherty and RobetVan Dyck [3] four features namely centroid,dispersion, grayscale distribution, object texture are used for tracking objects. The features are defined as follows: 
Centroid = (cx, cy ) where,
cx = Σ(pi,j * i )/ Σ( p i,j )   ----(5)
cy = Σ(pi,j * j )/ Σ( p i,j )   ----(6)
dispersion = ( Σ √( (I – cx)2 + ( j – cy )2)*pi,j ) / (Σpi,j) ----(7)
Grey scale distribution of the image is expressed in terms of grey scale range grm, mean of the higher 10% values grh and mean of lower 10% values grl. Texture of the object tx is defined by the mean of higher 10% values in the wavelet edge image. In current frame under consideration each useful object is assigned the feature vector. Based on the domain knowledge some expected ‗tracks‘ could be generated. Thus the tracking algorithm becomes finding the best track for each object. For this a matrix X of m objects versus n tracks is computed. An element X[i][j] is the number of features that ‗match‘ with the observed features based on some threshold. Further for matrix a threshold for eligible tracks is set. If in a row there is only one track satisfying the eligibility threshold then it become  the current track of the object. The row and column corresponding to the object and the track is removed from the matrix. For objects with multiple possible tracks, weights are given to the features to evaluate ‗cost‘ for each track. The track that gives the least cost is assigned to the object. Here the weights are given purely on the basis of domain knowledge or based on some heuristic about usefulness of the feature. General procedure for processing a dynamic background (video) sequence is as follows:
1. Initialize mixture models for each pixel with a weak prior;
2. For each new frame:
              2.1 Update the estimated mixture model for each pixel using incremental EM;
              2.2 Heuristically label the mixture components;
              2.3 Classify each pixel according the current mixture model.
                                                                                            
CODING:
 1. Adding the API
Add the net.sourceforge.tess4j.*; API to your pom.xml:
<dependency><groupId>net.sourceforge.tess4j</groupId><artifactId>tess4j</artifactId><version>3.2.1</version></dependency>
This is the image that we're extracting the text from:
 
2. Download the CAPTCHA Language Extractor
Download the CAPTCHA language extractor and put it in the tessdata folder.
For example, if you download eng.trainedata from the above URL, put the file at the project root folder tessdata/eng-trainedata .


